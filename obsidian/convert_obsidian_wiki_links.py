"""
éœ€æ±‚ï¼š

å°† Obsidian ç¬”è®°ä¸­çš„å†…éƒ¨é“¾æ¥æ ¼å¼è½¬æ¢ä¸º Markdown é“¾æ¥æ ¼å¼

å¤„ç† Obsidian æ–‡æ¡£ä¸­çš„å†…éƒ¨é“¾æ¥æ ¼å¼èŒƒå›´ï¼š

1. Obsidian æ”¯æŒçš„ Wiki é“¾æ¥æ ¼å¼

- å½“å‰æ–‡ä»¶å†…é”šç‚¹é“¾æ¥ï¼š`[[#æ ‡é¢˜]]`     
- å½“å‰æ–‡ä»¶å†…å—æ ‡è¯†ç¬¦é“¾æ¥ï¼š`[[#^å—æ ‡è¯†ç¬¦]]`
- æ™®é€šæ–‡ä»¶é“¾æ¥ï¼š`[[assets/file1.md]]`
- æ”¯æŒæ–‡ä»¶å¸¦é”šç‚¹ï¼š`[[assets/file2.md#æ ‡é¢˜]]`
- æ”¯æŒæ–‡ä»¶å¸¦å—æ ‡è¯†ç¬¦ï¼š`[[assets/file3.md#^å—æ ‡è¯†ç¬¦]]`
- æ”¯æŒæ–‡ä»¶å¸¦åˆ«åï¼š`[[assets/file4.md|åˆ«å]]`
- æ”¯æŒæ–‡ä»¶å¸¦é”šç‚¹å’Œåˆ«åï¼š`[[assets/file5.md#æ ‡é¢˜|åˆ«å]]`
- æ”¯æŒæ–‡ä»¶å¸¦å—æ ‡è¯†ç¬¦å’Œåˆ«åï¼š`[[assets/file6.md#^å—æ ‡è¯†ç¬¦|åˆ«å]]`
- å›¾ç‰‡èµ„æºé“¾æ¥ï¼š`![[assets/image1.png]]`
- æ”¯æŒå›¾ç‰‡å¸¦å°ºå¯¸å£°æ˜ï¼š`![[assets/image2.png | 400x300]]`
- æ”¯æŒå›¾ç‰‡ä»…æŒ‡å®šå®½åº¦ï¼š`![[assets/image3.png | 400]]`

2. Obsidian ç‰¹æ®ŠåµŒå…¥æ ¼å¼

- å½“å‰æ–‡ä»¶å†…é”šç‚¹åµŒå…¥ï¼š`![[#æ ‡é¢˜]]`
- å½“å‰æ–‡ä»¶å†…å—æ ‡è¯†ç¬¦åµŒå…¥ï¼š`![[#^å—æ ‡è¯†ç¬¦]]`
- åµŒå…¥æ–‡ä»¶å†…å®¹ï¼š`![[assets/file10.md]]`
- åµŒå…¥ PDF é¡µé¢æŒ‡å®šé¡µæ•°ï¼š`![[assets/doc.pdf#page=3]]`


è½¬æ¢åçš„ Markdown é“¾æ¥æ ¼å¼ï¼š

1. æ ‡å‡† Markdown é“¾æ¥æ ¼å¼

- å½“å‰æ–‡ä»¶å†…é”šç‚¹é“¾æ¥ï¼š`[åˆ«å](#æ ‡é¢˜)`
- æ™®é€šæ–‡ä»¶é“¾æ¥ï¼š`[åˆ«å](assets/file7.md)`
- æ”¯æŒæ–‡ä»¶å¸¦é”šç‚¹ï¼š`[åˆ«å](assets/file8.md#æ ‡é¢˜)`
- å›¾ç‰‡èµ„æºé“¾æ¥ï¼š`![æè¿°](assets/image4.png)`
- æ™®é€šèµ„æºé“¾æ¥æŒ‡å‘å›¾ç‰‡ï¼š`[æè¿°](assets/image5.png)`
   
2. Obsidian Markdown æ‰©å±•

- å½“å‰æ–‡ä»¶å†…é”šç‚¹åµŒå…¥ï¼š`![åˆ«å](#æ ‡é¢˜)`
- å½“å‰æ–‡ä»¶å†…å—æ ‡è¯†ç¬¦åµŒå…¥`![åˆ«å](#^å—æ ‡è¯†ç¬¦)`
- å½“å‰æ–‡ä»¶å†…å—æ ‡è¯†ç¬¦é“¾æ¥ï¼š`[åˆ«å](#^å—æ ‡è¯†ç¬¦)`
- æ”¯æŒæ–‡ä»¶å¸¦å—æ ‡è¯†ç¬¦ï¼š`[åˆ«å](assets/file9.md#^å—æ ‡è¯†ç¬¦)`
- æ”¯æŒå›¾ç‰‡å¸¦å°ºå¯¸å£°æ˜ï¼š`![400x300](assets/image6.png)`
- æ”¯æŒå›¾ç‰‡å¸¦æè¿°å’Œå°ºå¯¸å£°æ˜ï¼š`![æè¿° | 400x300](assets/image7.png)`
- æ”¯æŒå›¾ç‰‡å¸¦æè¿°å’Œä»…å®½åº¦å£°æ˜ï¼š`![æè¿° | 400](assets/image8.png)`

"""
import os
import shutil
import re
from urllib.parse import urlparse, unquote, quote
import sys
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger('ObsidianLinkConverter')

# é…ç½®è·¯å¾„
source_folder = "Default"
source_note_dir = fr'D:\Obsidian\Default'
target_note_dir = fr'D:\Obsidian\Middle\wikiformat'
internal_link_prefix = r''  # å†…éƒ¨é“¾æ¥å‰ç¼€
# external_link_prefix = r''  # å¤–éƒ¨é“¾æ¥å‰ç¼€
# external_link_prefix = r'https://raw.githubusercontent.com/littlekj/linkres/master/obsidian/'
external_link_prefix = '/'  # ç›¸å¯¹åœ°å€å‰ç¼€æ·»åŠ  / ç”Ÿæˆç»å¯¹è·¯å¾„ï¼Œæ‹¼æ¥ GitHub ä»“åº“åœ°å€ä¾¿äº Web è®¿é—®

# Obsidian æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
supported_extensions = {
    'markdown': ['md'],
    'image': ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'avif', 'webp', 'svg'],
    'audio': ['flac', 'm4a', 'mp3', 'ogg', 'wav', 'webm', '3gp'],
    'video': ['mkv', 'mov', 'mp4', 'ogv', 'webm', 'avi'],
    'pdf': ['pdf']
}

# æ„å»ºæ‰€æœ‰æ‰©å±•åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
all_extensions = []
for category in supported_extensions.values():
    all_extensions.extend(category)
extensions_pattern = '|'.join(all_extensions)

# å…¨å±€èµ„æºç¼“å­˜ï¼ˆé¿å…é‡å¤æŸ¥æ‰¾ï¼‰
resource_cache = {}


# åŒ¹é…å†…è”ä»£ç  å’Œ å¤šè¡Œä»£ç å—ï¼ˆåå¼•å·/æ³¢æµªå·ï¼Œ3ä¸ªæˆ–ä»¥ä¸Šï¼‰
# æ”¹è¿›çš„æ­£åˆ™ï¼šä¸ºæ¯ç§æƒ…å†µè®¾ç½®æ•è·ç»„ï¼Œå¹¶ç¡®ä¿å†…å®¹è¢«æ•è·
CODE_PATTERN = re.compile(
    r'(`[^`]+?`)'                                  # group 1: å†…è”ä»£ç 
    r'|(~{3,})([a-zA-Z][\w-]*)?\s*\n'              # group 2: æ³¢æµªå·å¼€å§‹, group 3: è¯­è¨€
    r'([\s\S]*?)\n'                                # group 4: æ³¢æµªå·å†…å®¹
    r'(~{3,})(?=\n|$)'                             # group 5: æ³¢æµªå·ç»“æŸ
    r'|(`{3,})([a-zA-Z][\w-]*)?\s*\n'              # group 6: åå¼•å·å¼€å§‹, group 7: è¯­è¨€
    r'([\s\S]*?)\n'                                # group 8: åå¼•å·å†…å®¹
    r'(`{3,})(?=\n|$)',                            # group 9: åå¼•å·ç»“æŸ
    re.MULTILINE
)


def save_code_blocks(content):
    code_blocks = []
    placeholder_counter = 0

    def replace_func(match):
        nonlocal placeholder_counter
        placeholder_counter += 1
        placeholder = f"__CODE_BLOCK_{placeholder_counter}__"

        if match.group(1):  # å†…è”ä»£ç 
            code = match.group(1)
        elif match.group(2):  # æ³¢æµªå·ä»£ç å—
            start_delim = match.group(2)   # ~~~
            lang = match.group(3) or ""    # å¯é€‰è¯­è¨€
            body = match.group(4)
            end_delim = match.group(5)     # ~~~
            # ä¿ç•™è¯­è¨€æ ‡è¯†
            code = f"{start_delim}{lang}\n{body}\n{end_delim}"
        else:  # åå¼•å·ä»£ç å—
            start_delim = match.group(6)   # ```
            lang = match.group(7) or ""    # å¯é€‰è¯­è¨€
            body = match.group(8)
            end_delim = match.group(9)     # ```
            # ä¿ç•™è¯­è¨€æ ‡è¯†
            code = f"{start_delim}{lang}\n{body}\n{end_delim}"

        code_blocks.append((placeholder, code))
        return placeholder

    new_content = CODE_PATTERN.sub(replace_func, content)
    return new_content, code_blocks


def restore_code_blocks(content, code_blocks):
    """
    å°†å ä½ç¬¦æ›¿æ¢å›åŸå§‹ä»£ç å—
    """
    for placeholder, code in code_blocks:
        content = content.replace(placeholder, code)
    return content


# Wiki é“¾æ¥æ­£åˆ™ï¼ˆæ”¯æŒè·¯å¾„/æ ‡é¢˜/å—/å°ºå¯¸/åˆ«åï¼Œç«–çº¿å‰åå¯æœ‰ç©ºæ ¼ï¼‰
wiki_link_regex = r"""
    (!?)                           # 1: å¯é€‰ "!"ï¼ˆembedï¼‰
    \[\[
        (?:([^\]\|\n#^]+?)\s*)?    # 2: è·¯å¾„ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨å»æ‰å°¾ç©ºæ ¼ï¼‰
        (?:\#(?:
            (?!\^)([^\]\|\n#^]+)   # 3: æ ‡é¢˜ï¼ˆ#xxxï¼‰
          | \^([^\]\|\n#]+)        # 4: å—æ ‡è¯†ç¬¦ï¼ˆ#^xxxï¼‰
        ))?
        (?:\s*\|\s*(\d{1,4}(?:x\d{1,4})?))?   # 5: å°ºå¯¸ï¼ˆ400 æˆ– 400x300ï¼‰
        (?:\s*\|\s*([^\]\n|]+))?              # 6: åˆ«å
    \]\]
"""

wiki_link_pattern = re.compile(wiki_link_regex, re.VERBOSE)


def parse_desc_size(raw_desc_or_size, size_group):
    """è§£æå›¾ç‰‡æè¿°å’Œå°ºå¯¸"""
    if not size_group:
        if raw_desc_or_size and re.match(r'^\d{1,4}(?:x\d{1,4})?$', raw_desc_or_size):
            return None, raw_desc_or_size
        return raw_desc_or_size, None

    return raw_desc_or_size, size_group


# å¸¸è§é¡¶çº§åŸŸåï¼ˆTLDï¼‰ï¼Œç”¨äºåŒºåˆ† Web é“¾æ¥å’Œæœ¬åœ°æ–‡ä»¶
COMMON_TLDS = {
    # é€šç”¨
    'com', 'org', 'net', 'edu', 'gov', 'mil', 'int', 'biz', 'info', 'name', 'pro',
    'museum', 'coop', 'aero', 'post', 'geo', 'kid', 'law', 'mail', 'sco', 'web',
    # å›½å®¶
    'cn', 'uk', 'de', 'fr', 'jp', 'au', 'ca', 'ru', 'in', 'br', 'it', 'es', 'nl',
    # æ–°é€šç”¨
    'app', 'dev', 'io', 'ai', 'co', 'tv', 'xyz', 'online', 'site', 'store', 'tech',
    'cloud', 'space', 'blog', 'news', 'wiki', 'shop', 'bank', 'sport', 'game',
    'music', 'movie', 'photo', 'art', 'design', 'studio', 'today', 'world',
    # å…¶ä»–å¸¸è§
    'us', 'uk', 'eu', 'me', 'tv', 'cc', 'la', 'pw', 'info', 'mobi',
}

# æ–‡ä»¶æ‰©å±•åé»‘åå•ï¼ˆæ˜ç¡®ä¸æ˜¯ TLD çš„ï¼‰
FILE_EXTS = {
    'pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx',
    'txt', 'md', 'markdown', 'rtf', 'log',
    'jpg', 'jpeg', 'png', 'gif', 'bmp', 'svg', 'webp',
    'zip', 'rar', '7z', 'tar', 'gz', 'iso',
    'exe', 'dll', 'bin', 'apk', 'pkg',
    'mp3', 'wav', 'flac', 'mp4', 'avi', 'mkv', 'mov',
    'css', 'js', 'json', 'xml', 'html', 'htm',
    'py', 'java', 'cpp', 'c', 'h', 'go', 'rs', 'ts', 'sh',
    'tmp', 'bak', 'old', 'swp', 'lock',
}

def is_web_link(link: str) -> bool:
    """
    åˆ¤æ–­é“¾æ¥æ˜¯å¦ä¸ºç½‘é¡µé“¾æ¥ï¼ˆå¤–éƒ¨ç½‘ç»œèµ„æºï¼‰
    
    ç­–ç•¥ï¼š
    1. å…ˆæ’é™¤æ˜ç¡®çš„æœ¬åœ°é“¾æ¥
    2. å†åˆ¤æ–­æ˜ç¡®çš„ Web é“¾æ¥
    3. æœ€åç”¨ TLD + æ ¼å¼åˆ¤æ–­æ¨¡ç³Šæƒ…å†µ
    """
    if not isinstance(link, str) or not link.strip():
        return False
    link = link.strip()
    if not link:
        return False

    # 1. æ˜ç¡®çš„æœ¬åœ°æˆ–ç‰¹æ®Šåè®®é“¾æ¥ â†’ é Web
    private_ipprivate_ip_pattern = re.compile(
        r'\b127\.0\.0\.1\b'                    # å›ç¯åœ°å€
        r'|\b192\.168\.\d+\.\d+\b'             # 192.168.x.x
        r'|\b10\.\d+\.\d+\.\d+\b'              # 10.x.x.x
        r'|\b172\.(1[6-9]|2[0-9]|3[01])\.\d+\.\d+\b'  # 172.16.0.0 ~ 172.31.255.255
    )
    if (
        link.startswith('obsidian://') or
        link.startswith('file://') or
        'localhost' in link.lower() or
        private_ipprivate_ip_pattern.search(link)
    ):
        return False

    # 2. åè®®å¤´æ˜ç¡®çš„ Web é“¾æ¥
    if link.startswith(('http://', 'https://', 'ftp://', 'mailto:', 'tel:')):
        return True

    # 3. åè®®ç›¸å¯¹é“¾æ¥ï¼ˆ//example.comï¼‰
    if link.startswith('//'):
        return True

    # 4. ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ â†’ æœ¬åœ°è·¯å¾„é“¾æ¥
    if link.startswith(('./', '../', '/')) or '\\' in link or link.startswith('\\\\'):
        return False
    
    # 5. çº¯æ–‡ä»¶ååˆ¤æ–­ï¼ˆä¼˜å…ˆäºåŸŸååˆ¤æ–­ï¼‰
    # å¦‚æœæ˜¯ xxx.yyy æ ¼å¼ï¼Œä¸” yyy ä¸æ˜¯å¸¸è§ TLDï¼Œåˆ™è§†ä¸ºæ–‡ä»¶
    filename_match = re.match(r'^[a-zA-Z0-9][a-zA-Z0-9._-]*\.([a-zA-Z0-9]{2,6})$', link)
    if filename_match:
        ext = filename_match.group(1).lower()
        # å¦‚æœæ‰©å±•ååœ¨æ–‡ä»¶é»‘åå•ä¸­ â†’ æœ¬åœ°
        if ext in FILE_EXTS:
            return False
        # å¦‚æœæ‰©å±•åæ˜¯å…¬è®¤ TLD â†’ Web
        if ext in COMMON_TLDS:
            return True
        # æ¨¡ç³Šæƒ…å†µï¼šä¸åœ¨ TLD åˆ—è¡¨ä¸­ â†’ å€¾å‘äºæœ¬åœ°ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        return False

    # 6. ä¸¥æ ¼åŸŸåæ ¼å¼ + TLD æ£€æŸ¥
    # ä¿®æ”¹æ­£åˆ™ï¼šæ˜ç¡®æ•è· TLD
    domain_pattern = re.compile(
        r'^'
        r'(?:[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)*'  # å­åŸŸï¼ˆå¯é€‰ï¼‰
        r'[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?'         # ä¸»åŸŸåï¼ˆå¦‚ exampleï¼‰
        r'\.'                                                  # å¿…é¡»æœ‰ä¸€ä¸ªç‚¹
        r'[a-zA-Z]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?'            # é¡¶çº§åŸŸï¼ˆå¦‚ com, org, xn--ï¼‰
        r'(?::\d{1,5})?'                                       # å¯é€‰ç«¯å£ï¼ˆ:8080ï¼‰
        r'(?:/[^\s]*)?'                                        # å¯é€‰è·¯å¾„ï¼ˆ/path/to/pageï¼‰
        r'$', re.IGNORECASE
    )
    if domain_pattern.match(link):
        tld = link.split('.')[-1].lower()
        if tld in COMMON_TLDS:
            return True

    # 7. å…¶ä»–æƒ…å†µè§†ä¸ºæœ¬åœ°é“¾æ¥
    return False


def extract_wiki_links(text):
    """Obsidian Wiki é“¾æ¥è§£æ"""
    matches = []
    for match in wiki_link_pattern.finditer(text):
        path = match.group(2) or None
        # isImage = path and is_image(match.group(2))
        # print("image_path:", match.group(2))
        full_match = match.group(0)
        embed = bool(match.group(1))
        title = match.group(3)
        block_id = match.group(4)
        desc = match.group(6)
        size = match.group(5)

        matches.append({
            'full_match': full_match,
            'type': 'wiki',
            'embed': embed,
            'path': path,
            'title': title,
            'block_id': block_id,
            'desc': desc,
            'size': size,
            'start': match.start(),
            'end': match.end(),
        })
            
    return matches


def confirm_delete(path):
    """ç¡®è®¤æ˜¯å¦åˆ é™¤æŒ‡å®šè·¯å¾„"""
    confirm = input(f"âš ï¸  ç¡®è®¤åˆ é™¤å†…å®¹ï¼š{path}ï¼Ÿ(y/N): ").strip().lower()
    return confirm == 'y'    

def remove_if_exists(path):
    """åˆ é™¤æ–‡ä»¶æˆ–ç›®å½•ï¼Œå¦‚æœå­˜åœ¨"""
    if os.path.exists(path):
        if os.path.isfile(path):
            os.remove(path)
        elif os.path.isdir(path):
            shutil.rmtree(path)
        logger.info(f"å·²åˆ é™¤: {path}")

def safe_remove_if_exists(path):
    """å®‰å…¨åˆ é™¤ç›®å½•ï¼Œå…ˆç¡®è®¤å†æ‰§è¡Œ"""
    if confirm_delete(path):
        remove_if_exists(path)
    else:
        print("âŒ å·²å–æ¶ˆåˆ é™¤æ“ä½œã€‚")  
        sys.exit(1)  # ç«‹å³é€€å‡ºç¨‹åº


def copy_files(source_note_dir, ignored_extensions=None):
    """å¤åˆ¶æºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•"""
    ignored_extensions = ignored_extensions or []
    for item in os.listdir(source_note_dir):
        source_path = os.path.join(source_note_dir, item)
        destination_path = os.path.join(target_note_dir, item)

        # è·³è¿‡å¿½ç•¥çš„æ–‡ä»¶ç±»å‹
        if any(source_path.endswith(ext) for ext in ignored_extensions):
            continue

        # # è·³è¿‡ç‰¹å®šç³»ç»Ÿæ–‡ä»¶
        # if item.startswith('.') or item in ['Thumbs.db', 'desktop.ini']:
        #     continue

        if os.path.isdir(source_path):
            shutil.copytree(source_path, destination_path, dirs_exist_ok=True)
            logger.info(f"å¤åˆ¶ç›®å½•: {source_path} -> {destination_path}")
        else:
            shutil.copy2(source_path, destination_path)
            logger.info(f"å¤åˆ¶æ–‡ä»¶: {source_path} -> {destination_path}")


def copy_files_with_timestamps(source_note_dir, ignored_extensions=None):
    """å¤åˆ¶æºç›®å½•ä¸­æ‰€æœ‰æ–‡ä»¶åˆ°ç›®æ ‡ï¼Œå¹¶ä¿ç•™åŸå§‹æ—¶é—´æˆ³"""
    try:
        from copy_with_timestamps import copy_with_timestamps
    except ImportError:
        logger.error("æ— æ³•å¯¼å…¥ copy_with_timestamps æ¨¡å—ï¼Œè¯·ç¡®ä¿æ¨¡å—å­˜åœ¨ã€‚")
    
    ignored_extensions = ignored_extensions or []
    for item in os.listdir(source_note_dir):
        source_path = os.path.join(source_note_dir, item)
        if item.startswith('.') and os.path.isfile(source_path):
            # éšè—æ–‡ä»¶å¤åˆ¶æ—¶ï¼Œä¸åœ¨å¤åˆ¶å‘½ä»¤çš„ç›®æ ‡è·¯å¾„ä¸­æŒ‡å®šæ–‡ä»¶
            destination_path = os.path.join(target_note_dir)
            # print("destination_path", destination_path)
        else:
            destination_path = os.path.join(target_note_dir, item)
        
        # è·³è¿‡å¿½ç•¥çš„æ–‡ä»¶ç±»å‹
        if any(source_path.endswith(ext) for ext in ignored_extensions):
            continue
        
        # remove_if_exists(destination_path)
        if os.path.isdir(source_path):
            copy_with_timestamps(source_path, destination_path)
            logger.info(f"å¤åˆ¶ç›®å½•ï¼š{source_path} -> {destination_path}")
        else:
            copy_with_timestamps(source_path, destination_path)
            logger.info(f"å¤åˆ¶æ–‡ä»¶ï¼š{source_path} -> {destination_path}")


def get_ignore_list(target_dir):
    """è·å–å¿½ç•¥æ–‡ä»¶åˆ—è¡¨"""
    ignore_files_path = os.path.join(target_dir, '.gitignore')
    if not os.path.exists(ignore_files_path):
        return []

    ignored = []
    with open(ignore_files_path, 'r', encoding='utf-8', newline='') as f:
        for line in f:
            stripped_line = line.strip()
            if stripped_line and not stripped_line.startswith('#'):
                # å¤„ç†ç›®å½•å¿½ç•¥ï¼ˆç§»é™¤ç»“å°¾çš„/ï¼‰
                if stripped_line.endswith('/'):
                    stripped_line = stripped_line[:-1]
                ignored.append(stripped_line)
    return ignored


def find_resource_file(source_dir, resource_path, current_note_dir):
    """
    åœ¨ä»“åº“ä¸­æŸ¥æ‰¾èµ„æºæ–‡ä»¶ï¼Œæ”¯æŒå„ç§ç›¸å¯¹è·¯å¾„æ ¼å¼
    :param source_dir: ä»“åº“æ ¹ç›®å½•
    :param resource_path: èµ„æºè·¯å¾„ï¼ˆå¯èƒ½åŒ…å«ç›¸å¯¹è·¯å¾„ï¼‰
    :param current_note_dir: å½“å‰ç¬”è®°æ‰€åœ¨ç›®å½•
    :return: åŸºäºä»“åº“æ ¹ç›®å½•çš„ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å›None
    """
    # è½¬æ¢URLç¼–ç çš„ç©ºæ ¼ä¸ºæ™®é€šç©ºæ ¼
    resource_path = decode_url_space_only(resource_path)

    # æ£€æŸ¥ç¼“å­˜
    cache_key = (resource_path, current_note_dir)
    if cache_key in resource_cache:
        return resource_cache[cache_key]

    # å°è¯•å¯èƒ½çš„è·¯å¾„ç»„åˆ
    possible_paths = []

    # ç›¸å¯¹äºå½“å‰ç¬”è®°çš„è·¯å¾„
    relative_to_note = os.path.join(current_note_dir, resource_path)
    possible_paths.append(relative_to_note)
    
    # ç›¸å¯¹äºä»“åº“æ ¹ç›®å½•çš„è·¯å¾„
    relative_to_root = os.path.join(source_dir, resource_path)
    possible_paths.append(relative_to_root)
    
    # å°è¯•è§£æç»å¯¹è·¯å¾„ï¼ˆä»¥ / å¼€å¤´ï¼‰
    if resource_path.startswith('/'):
        abs_path = os.path.abspath(os.path.join(source_dir, resource_path[1:]))
        possible_paths.append(abs_path)
        
    # å°è¯•è§£æç›¸å¯¹è·¯å¾„ï¼ˆä»¥ ./ æˆ– ../ å¼€å¤´ï¼‰
    elif resource_path.startswith(('./', '../')):
        abs_path = os.path.abspath(os.path.join(current_note_dir, resource_path))

        # ç¡®ä¿è·¯å¾„åœ¨ä»“åº“æ ¹ç›®å½•å†…
        if not abs_path.startswith(os.path.abspath(source_dir)):
            logger.warning(f"èµ„æºè·¯å¾„è¶…å‡ºä»“åº“èŒƒå›´ï¼š{abs_path}")
            resource_cache[cache_key] = None
            return None

        possible_paths.append(abs_path)
        
    # å°è¯•è§£æå…¶ä»–ç›¸å¯¹è·¯å¾„
    else:
        # å°è¯•ç›¸å¯¹äºå½“å‰ä»“åº“çš„ç›¸å¯¹è·¯å¾„
        direct_path = os.path.normpath(os.path.join(source_dir, resource_path))
        possible_paths.append(direct_path)
        
        # å°è¯•ç›¸å¯¹äºå½“å‰ç¬”è®°çš„éšå¼ç›¸å¯¹è·¯å¾„
        abs_path = os.path.normpath(os.path.join(current_note_dir, resource_path))
        possible_paths.append(abs_path)
        
    for path in possible_paths:
        # åˆ¤æ–­è·¯å¾„æ˜¯å¦ä¸ºæ–‡ä»¶
        if os.path.isfile(path):
            rel_path = os.path.relpath(path, source_dir)
            resource_cache[cache_key] = rel_path
            return rel_path
        # æ–‡ä»¶åå½¢å¦‚ï¼šfile.ext.extï¼Œä½†æ’å…¥çš„å¯èƒ½æ˜¯ file.ext
        # å°è¯•ç›´æ¥æ·»åŠ æ‰©å±•å
        else:
            for ext in all_extensions:
                extended_path = f"{path}.{ext}"
                if os.path.isfile(extended_path):
                    rel_path = os.path.relpath(extended_path, source_dir)
                    resource_cache[cache_key] = rel_path
                    return rel_path

    # å°è¯•å…¨åº“æ–‡ä»¶åæœç´¢     
    filename = os.path.basename(resource_path)
    for root, _, files in os.walk(source_dir):
        for file in files:
            # åŒ¹é…æ–‡ä»¶åï¼ˆå¸¦æ‰©å±•åæˆ–ä¸å¸¦æ‰©å±•åï¼‰
            if file == filename or any(file == f"{filename}.{ext}" for ext in all_extensions):
                file_path = os.path.join(root, file)
                if os.path.isfile(file_path):
                    rel_path = os.path.relpath(file_path, source_dir)
                    resource_cache[cache_key] = rel_path
                    return rel_path

    # æœªæ‰¾åˆ°èµ„æº
    resource_cache[cache_key] = None
    return None


def get_file_type(file_path):
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–æ–‡ä»¶ç±»å‹"""
    ext = file_path.split('.')[-1].lower() if '.' in file_path else ''
    for file_type, extensions in supported_extensions.items():
        if ext in extensions:
            return file_type
    return 'other'


def encode_url_space_only(url):
    """
    ä»…å¯¹URLä¸­çš„ç©ºæ ¼è¿›è¡Œç¼–ç 
    """
    return url.replace(" ", "%20")

def decode_url_space_only(url):
    """
    ä»…å¯¹URLä¸­çš„ç©ºæ ¼è¿›è¡Œè§£ç 
    """
    return url.replace("%20", " ")


def convert_wiki_links(note_file_path, updated_content):
    """
    å°†æ–‡ä»¶ä¸­çš„ Obsidian Wiki é“¾æ¥è½¬æ¢ä¸º Markdown è¶…é“¾æ¥æ ¼å¼
    :param note_file_path: ç¬”è®°æ–‡ä»¶è·¯å¾„
    :param updated_content: ç¬”è®°å†…å®¹
    """
    # å½“å‰ç¬”è®°æ‰€åœ¨ç›®å½•
    current_note_dir = os.path.dirname(note_file_path)
    
    # éå†æ‰€æœ‰åŒ¹é…åˆ°çš„é“¾æ¥
    matches = extract_wiki_links(updated_content)
    
    # print("matches:", matches)
    # æŒ‰é“¾æ¥åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®æ’åº
    matches.sort(key=lambda m: m['start'])
    
    parts = []  # ç”¨äºå­˜å‚¨å¤„ç†åçš„å†…å®¹ç‰‡æ®µ
    last_end = 0   # è®°å½•ä¸Šä¸€æ¬¡åŒ¹é…ç»“æŸçš„ä½ç½®
    
    if matches:
        for match in matches:
            print('full_match:', match['full_match'])
            parts.append(updated_content[last_end:match['start']])
            resource_path = match['path']
 
            if not resource_path:
                resource_path = note_file_path
                
            resource_name = os.path.basename(resource_path)
            resource_relpath = find_resource_file(target_note_dir, resource_path, current_note_dir)
              
            if resource_relpath:
                # è®¡ç®—ç›¸å¯¹ä»“åº“æ ¹ç›®å½•çš„è·¯å¾„
                rel_path = resource_relpath.replace('\\', '/')  # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ 
                print('rel_path:', rel_path)
                
                # æ‹¼æ¥æˆå®Œæ•´çš„ URL
                internal_link_prefix = r''
                full_url = f'{internal_link_prefix}{rel_path}'
                
                if match['embed']:
                    full_path = f'!['
                else:
                    full_path = f'['
                if not match['desc'] and not match['size']:
                    full_path += f'{resource_name}'
                elif match['desc']:
                    full_path += f'{match["desc"]}'
                    if match['size']:
                        full_path += f'|{match["size"]}'
                else:
                    full_path += f'{match["size"]}'
                full_path += f']('
                
                if match['title'] and not match['block_id']:
                    full_url += f'#{match["title"]}'
                if (not match['title']) and match['block_id']:
                    full_url += f'#^{match["block_id"]}'
                full_url = decode_url_space_only(full_url)
                full_url = encode_url_space_only(full_url)
                full_path += full_url + ')'   
            else:
                full_path = match['full_match']
                logger.warning(f"âš ï¸ è­¦å‘Š: èµ„æºæœªæ‰¾åˆ°ï¼š {resource_path}")
                logger.warning(f"ğŸ“ åœ¨ç¬”è®°ä¸­: {note_file_path}")
                logger.warning(f"â© æ­¤èµ„æºé“¾æ¥ï¼š{full_path}")
            
            # æ·»åŠ åŒ¹é…åˆ°çš„é“¾æ¥åˆ°å†…å®¹ç‰‡æ®µ
            parts.append(full_path)
            last_end = match['end']  # æ›´æ–°ä¸Šæ¬¡å¤„ç†ç»“æŸä½ç½®

        # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
        parts.append(updated_content[last_end:])

        # å°†æ‰€æœ‰ç‰‡æ®µé‡æ–°ç»„åˆæˆæ–°çš„å†…å®¹
        updated_content = ''.join(parts)

        return updated_content 
    
    return updated_content


def update_resource_links(note_file_path):
    """
    æ›´æ–°æ–‡ä»¶ä¸­çš„èµ„æºé“¾æ¥ä¸º Markdown è¶…é“¾æ¥æ ¼å¼
    :param note_file_path: ç¬”è®°æ–‡ä»¶è·¯å¾„
    """
    with open(note_file_path, 'r', encoding='utf-8', newline='') as file:
        try:
            content = file.read()
        except IOError as e:
            print(f"IOError: {e}")
        except UnicodeDecodeError as e:
            print(f"Unicode")
        except Exception as e:
            print(f"Unexpected error: {e}")
        
    # æå–ä»£ç å—å¹¶ç”¨å ä½ç¬¦æ›¿æ¢
    updated_content, code_blocks = save_code_blocks(content)
    
    # æå–æ‰€æœ‰èµ„æºé“¾æ¥è½¬æ¢ä¸º Markdown è¶…é“¾æ¥
    updated_content = convert_wiki_links(note_file_path, updated_content)
    
    # æ¢å¤ä»£ç å—
    updated_content = restore_code_blocks(updated_content, code_blocks)
    
    with open(note_file_path, 'w', encoding='utf-8', newline='') as file:
        try:
            file.write(updated_content)
        except Exception as e:
            logger.error(f"Error writing to file: {e}")
    

def iterate_files(target_note_dir):
    """éå†ç›®æ ‡ç›®å½•ä¸­çš„æ‰€æœ‰ç¬”è®°æ–‡ä»¶æ›´æ–°é“¾æ¥"""
    ignored_dirs = get_ignore_list(target_note_dir)
    updated_count = 0
    for root, dirs, files in os.walk(target_note_dir):
        # æ’é™¤ç‰¹å®šå­ç›®å½•
        dirs[:] = [d for d in dirs if d not in ignored_dirs]

        for file in files:
            if file.endswith('.md'):
                note_file_path = os.path.join(root, file)
                updated_count += 1
                logger.info(f"å¤„ç†ç¬”è®°: {note_file_path}")
                update_resource_links(note_file_path)
                
    return updated_count


def main():
    """æ‰§è¡Œæ–‡ä»¶å¤åˆ¶å’Œæ›´æ–°æ“ä½œ"""
    # ç¡®è®¤åˆ é™¤ç›®æ ‡ç›®å½•
    safe_remove_if_exists(target_note_dir)
    # åˆ›å»ºæ–°ç›®å½•
    os.makedirs(target_note_dir, exist_ok=True)
    
    logger.info("å¼€å§‹å¤„ç†...")
    logger.info(f"æºç›®å½•: {source_note_dir}")
    logger.info(f"ç›®æ ‡ç›®å½•: {target_note_dir}")

    # å¤åˆ¶æ–‡ä»¶ï¼ˆå¿½ç•¥ç‰¹å®šæ‰©å±•åï¼‰
    ignored_extensions = ['.tmp', '.DS_Store']
    # copy_files(source_note_dir, ignored_extensions)
    copy_files_with_timestamps(source_note_dir, ignored_extensions)

    # æ›´æ–°ç¬”è®°ä¸­çš„èµ„æºé“¾æ¥
    updated_count = iterate_files(target_note_dir)

    logger.info("\nâœ… å¤„ç†å®Œæˆï¼")
    logger.info(f"å…±å¤„ç† {updated_count} ä¸ªç¬”è®°: {target_note_dir}")


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.setLevel(logging.INFO)
    main()
